Functional Requirements – Claims Data Centralization and Reporting
1. Data Ingestion
Ingest data from multiple systems (e.g., Policy, Claim, Finance, Reinsurance).

Identify which system provides which type of data (policy-level, claim-level).

Allow new data sources to be added easily.

2. Document Extraction (ImageRight PDFs)
Extract fields from PDF documents like invoices, loss notices.

Link each field to the PDF it came from.

Maintain a mapping table for traceability.

3. Data Transformation
Standardize formats like dates, currency, status codes across sources.

Apply rules to calculate premium, claim, deductible amounts.

Allow logic to be applied after data is centralized.

Support yearly treaty logic (10–12 new treaties per year) through configuration.

4. Currency Conversion and Rounding
Convert currencies using exchange rate tables.

Handle rounding differences and log mismatches.

Skip conversion if broker already provided final values.

5. Reconciliation
Match policy premium with actual financial transactions.

Match system data with broker data and flag differences.

Log any mismatches with time, source, and details.

6. Modeling Features
Identify target variable (e.g., claim paid or not) for ML models.

Prepare dataset for modeling (e.g., payment propensity).

Export model-ready data for data science teams.

7. Reporting and Dashboards
Create dashboards for claim trends, premium earned/unearned, treaty performance.

Support filters by line of business, geography, policyholder, etc.

8. Audit and Governance
Track every transformation applied to data.

Keep logs of rule version, who made changes, and when.

Ensure data is traceable from source to final output.

9. Treaty Configuration
Support different types of reinsurance treaties.

Store start/end date, deductible, retention, payout rules per treaty.

Load new treaty data each year and rerun calculations if needed.

$$$$$
Subject: Review of Client Inputs – Exposure & Experience Premium Logic & Data Structure
Hi [Project Manager's Name],

We hope you're doing well.

We have reviewed the latest document and information shared by the client regarding the exposure and experience premium pricing model. Below is a consolidated summary of our understanding, key observations, and areas where further clarification is needed to support the development process.

Overview of What Has Been Shared
The document shared by the client is structured into three main sections:

Page 1: Table & Attribute Mapping
This section lists the source tables and corresponding attributes intended for use in the premium calculation.

During our review, we observed:

Some attribute names mentioned do not exactly match those in the provided SQL tables.

It is unclear whether the missing attributes are renamed versions, derived fields, or exist in related tables.

Clarification is needed to confirm if all fields referenced in the calculation logic are available in the current datasets.

Page 2: Discounts and Other Inputs
This section covers discount/surcharge rules and refers to specific tables and attributes.

We noted that:

A few attributes are mentioned that are not present in the specified tables.

Several fields are marked as manually entered or sourced from Excel, but it’s not clear whether:

These inputs are required for exposure premium calculations.

They serve as temporary placeholders or are part of a manual override process.

It would be helpful to understand the logic or business reasoning behind these manually entered values.

Page 3: Exposure Premium Calculation (Excel-Based)
This section includes the full logic for exposure premium, implemented entirely using Excel formulas.

The logic appears to be complex, with multiple layered calculations.

Since the source data resides in SQL and our implementation is planned using SQL/Python, we recommend that this logic be shared in SQL format or as structured pseudocode.

This is particularly important because:

Translating complex Excel logic to SQL without sufficient clarity can introduce misinterpretation or errors.

The exposure premium is a critical target, and misalignment between logic and data can impact model accuracy.

A SQL-based version will ensure better traceability, reproducibility, and validation.

Additional Observations Based on Previous Discussions
The source tables are currently being populated through Excel uploads. However:

We have not received any documentation or SQL logic showing how these Excel files are ingested into the tables.

It remains unclear how the Excel columns are mapped to table fields and whether any transformation or validation steps are performed.

Experience premium logic has not been shared yet. When available, we recommend it also be shared in SQL or structured form.

For unstructured data (from ImageRight), we are still waiting for:

A complete list of attributes being extracted.

Clarification on where those extracted fields are being stored for downstream usage.

Note on Target Variables
As of now, the exposure premium and experience premium are the two confirmed target variables for modeling.
However, we understand that additional targets may be introduced in the future, as this remains an ongoing discussion with stakeholders. We will keep this in mind and remain flexible in our implementation plan to accommodate further requirements.

Summary of Follow-Up Needs from the Client
To enable smooth development and integration, the following clarifications are required:

SQL logic or ETL transformation steps for ingesting Excel data into the source tables.

Clarification on unmatched or missing fields referenced in pages 1 and 2.

A SQL-based version (or structured pseudocode) of the exposure premium logic.

Experience premium calculation logic (once available), preferably in SQL.

Attribute list for ImageRight (unstructured data) and confirmation of storage destination.

Confirmation on whether additional target metrics are expected, and how they may be defined.

Please let us know if you’d like us to send a follow-up to the client with these points, or if you prefer to wait for the next round of inputs. We’re happy to assist further as needed.

Best regards,
##########3
One critical point we would like to highlight is the need for clarity on the physician-level entity for whom the premiums and related calculations are being derived.

Since the exposure and experience premium logic is ultimately applied at the physician level, we assume there must be a structured way to identify and profile each physician, typically via the NPI (National Provider Identifier) or similar unique ID.

However, based on the current documentation and shared tables, it is not yet clear:

Which table(s) contain the physician-level records.

Whether NPI or another identifier is being used as the primary key.

How other datasets (e.g., claims, endorsements, submissions, discounts) are being linked back to individual physicians.

To move forward effectively with modeling and ensure proper target association, we request:

Confirmation of the physician reference/master table.

Schema and sample structure of this table.

Details on how all incoming data will be joined at the physician level using NPI or another consistent identifier.

This will help ensure that all predictions and premium calculations are properly aligned and interpretable at the physician granularity level.

####
We have received several premium-related factors such as:

Base Rate

Class Factor

Claim-Made Factor

Claim-Made Year

One-Year Adjustment Factor

Limit Factor

Class Layer Factor

Layer 3 ILF Factor

Charity Factor

Expense Factor

While we understand these are used in the exposure premium calculation, it would be extremely helpful to have a business-level explanation or definition of each of these terms.

Specifically, we would like to understand:

What each factor represents in the context of physician pricing or underwriting.

How they influence or adjust the final premium.

Whether any of these are standardized across the industry or specific to this product/organization.

If there are any rules, ranges, or constraints associated with these fields.

How these factors interact with other variables (e.g., physician specialty, limit, region).

Having this clarity will not only ensure proper model integration but also help us build transparent and explainable logic that aligns with business expectations.
